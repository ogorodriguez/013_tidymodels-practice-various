---
title: "Tidymodels: Tidy Machine Learning in R"
author: "LJ"
date: "2023-01-21"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      # cache = TRUE,
                      autodep = TRUE,
                      # cache.lazy = FALSE,
                      message = FALSE, 
                      include = TRUE,
                      dpi = 180,
                      fig.width = 8, 
                      fig.height = 5)

pacman::p_load(tidyverse,
               janitor,
               here,
               readxl,
               kableExtra,
               tidymodels,
               skimr)

ggplot2::theme_set(theme_minimal())

```

## Introduction

In this notebook I will review the website from Rebecca Barter on
`tidymodels.` I have learned purrr, and dplyr with this person's website
before. It excels in explaining R packages and their concepts in simple
ways.

[Rebecca Barter's post on
Tidymodels](https://www.rebeccabarter.com/blog/2020-03-25_machine_learning/)

She already tried to learn the `caret` package. This package was the
inception of `tidymodels` creator, Max Kuhn. The objective was to create
a unifed interface to perform machine learing in R.

`caret` was not a tidy package. `tidymodels` has resolved that and it
will subsitute other previous package.

Rebecca bases her study to learn `tidymodels` from the following
sources:

-   [Introduction to Maching Learning with the
    tidyverse](https://education.rstudio.com/blog/2020/02/conf20-intro-ml/)
-   [A Gentle introduction to
    tidymodels](https://education.rstudio.com/blog/2020/02/conf20-intro-ml/)

Sources that I will include in this learning phase.

## What is tidymodels?

`Tidymodels` consists of several packages, just like `tidyverse`. These
packages include:

-   `rsample` for sample splitting to get train, test and
    cross-validation data
-   `recipes` for pre-processing
-   `parsnip` for specifying the model
-   `yardstick` for evaluation the model

and there are [many more](https://www.tidymodels.org/packages/). The
suite can be loaded into R.\
The author will add other packages for tuning parameter, `tune`, and
`workflows` for putting it all together.

## Setting up the work

For the data to use in this notebook, we'll load the package `mlbench`.
This contains the data on Pima Indian Diabetes.

```{r loading the packages}
# Load the tidymodels and other packages
# tidymodels and tidyverse loaded at start
# From mlbench, we get the dataset to work with
pacman::p_load(workflows,
               tune,
               mlbench)

```

### The dataset

The data set to use is Pima Indian Women's diabetes. This dataset is
from the National Institute of Diabeter, Digestive, and Kidney Diseases.
It contains information on 768 indian women from Phoenix, Arizona, USA.

Interestingly, these indian people are called "Pima" because that is the
word they used when interacting with Spaish colinists a lot. "Pima" is
believed to mean "I don't know!"

The dataset include a number of prediction variables such as number of
pregnancies, BMI, age, diabetes pedigree, etc.

```{r rename and visualize the dataset}
data("PimaIndiansDiabetes")

diabetes_orig <- PimaIndiansDiabetes

diabetes_orig %>% 
  glimpse()

```

```{r view a random subset of the data}
diabetes_orig %>% 
  dplyr::slice_sample(n = 10)

```

```{r view a summary of the data}
diabetes_orig %>% 
  skimr::skim()

```

This dataset contains lots of 0's in sections where it is not plausible
(BMI for instance.) This can be attributed to missing values in the
dataset.

Let's check the tricep skin fold variable distribution, and the BMI
distribution to check the shape of the dataset.

```{r checking distibution of variable tricep}
diabetes_orig %>% 
  ggplot() +
    geom_histogram(aes(x = triceps))

```

```{r checking distibution of variable mass}
diabetes_orig %>% 
  ggplot() +
    geom_histogram(aes(x = mass))

```

We can definitely see the values marked as 0, so far from the
distribution of data. It can also be seen in other variables as well.

For this we will do a transformation of the dataset so that all 0's
become NAs

```{r data cleaning replace 0s with NAs}
diabetes_clean <- diabetes_orig %>% 
  mutate(across(c(triceps, glucose, pressure, insulin, mass),
         function(.x) {
           case_when(
             .x == 0 ~ as.numeric(NA),  # if the entry is 0, replace it for NA
             TRUE ~ .x)                 # otherwise leave it as it is
         }))

diabetes_clean %>% 
  dplyr::slice_sample(n = 10)

```

Now let's check our new clean dataset summary.

```{r}
diabetes_clean %>% 
  skim()
```

```{r}
diabetes_clean %>%
  glimpse()
```

Once our data is hopefully clean, we can now start doing our machine
learning analysis with the tidymodels package.

## Tidymodels Process

### Splitting the data: train/test

To do a model we need to separate the dataset into parts: one for
training the model (or models), one for validating the good model (when
training on many different models to see which one is best,) and one for
testing whether the model we validated is actually the good one.

The training data is used to fit our model and tune its parameters, the
testing data is used to evaluate our final model's performance.

The `rsample` package is used to create these split objects.

```{r creating the training/test split objects}
set.seed(234589)
# split the data into training (75%) and testing (25%)
diabetes_split <- rsample::initial_split(diabetes_clean,
                                         prop = 3/4)

diabetes_split

```

The training output above is telling us that we have split the set into
two groups: a. The training set has 576 observations b. The testing set
has 192 observations c. The last number is the number of observations
overall: 768

The training and testing sets can be extracted from the split object
using the corresponding functions `training()`, and `testing()`

```{r get the training and testing data splits separately}
# extract training and testing sets
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)

```

For parameter tuning, we will need to have various versions of the
training set. It helps to create a cross-validation version of the
training set using the `vfold_cv()` function.

```{r creating CV object from training data}
# creating CV object from training data
diabetes_cv <- rsample::vfold_cv(diabetes_train)

```

### Defining a recipe

Recipes allows you to specify the formula to apply to the predictor
variables to build the desire model. It allows you to accommodate or
revise those variables of that formula prior to the execution of the
model itself (this is called pre-processing.)

There various pre-processing steps that can be done to our variables.
Some of them are: normalization, imputation, PCA, etc. There are many
many pre-processing steps. Their selection will depend on the variables
we have on our set.

The list of pre-processing steps can be found here: link needed.

The formula usually has the format of outcome = f(predictors).

In our dataset the outcome is to determine whether the patients has
diabetes based on the observations collected, but more importantly, is
to determine whether these observations are the indicated ones to
predict such an outcome, how reliable are they to get a diabetes
diagnosis.

Creating a recipe then has two parts layered one on top of the other by
using the pipe.

1.  First we specify the formula using the `recipe()` function, to
    specify which one is the outcome variable and which variables to use
    as predictors to create our formula. In this case, we will use them
    all.

2.  Specify the pre-processing steps to do the the predictor variables.
    Other cases of pre-processing may be scaling, creating dummy
    variables, etc.

In this occasion we will do the following:

```{r defining the recipe}
# define the recipe
diabetes_recipe <-
  recipe(diabetes ~ .,                 # the formula outcome is diabetes and the "." is used to state that the rest of the columns will be the predictors.
  data = diabetes_clean) %>% 
  step_normalize(all_numeric()) %>%    # pre-processing step to make all predictors numeric
  step_impute_knn(all_predictors())    # pre-processing step to make impute the knn to all other columns

```

The steps used here to preprocess the data are called "Role selections".
They specify that we want to apply the step to all numeric variables and
all predictor variables. Check `?selections` to see other examples.

For detailed information on normalizing data and why it is useful,
[please see
here](https://machinelearningmastery.com/using-normalization-layers-to-improve-deep-learning-models/).
In short, normalization makes our data centered and scaled. Centered
refers to fact that our data will have a mean of 0 and a standard
deviation of 1.

For detailed information on the Knn (K-nearest neighbor imputation),
please [click
here.](https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/)
In short, this is used when the dataset has lots of missing values, and
we need to impute an estimated numeric value based on the values of
nearby observations.

The dataset used to create the recipe was `diabetes_clean`, our working
full dataset. Any of the other sets: the training, or split object could
have been used as well.\
It doesn't matter what is used. All the recipe takes from the data at
this point is the *names and roles* of the outcome and predictors. Once
the recipe is defined, it can then be applied to specific dataset later.

For larger datasets, the head of the data can then be used to pass the
recipe to smaller dataset to save time and memory.

If we print on the console the diabetes_recipe object, it will only show
how many variables we have used as outcome and how many as predictors.
It will also show us the pre-processing steps applied.

```{r show the diabetes_recipe object}
diabetes_recipe

```

As indicated before, the recipe was a apply to a dataset only to
identify which variable is the outcome, which will be used as
predictors, and which steps are needed to make the predictors ready for
analysis. Now, if we want to apply the recipe to another data set (the
training, or testing, etc.) we need to use other functions.

a.  `prep()` will apply the recipe to the new dataset
b.  `juice()` will extract the pre-processed data. Meaning, it will show
    us how our dataset will look after the application of the recipe
    steps.

```{r application of the recipe using prep() to the training data, and looking at the pre-processed data with juice()}
diabetes_train_preprocessed <- 
  diabetes_recipe %>% 
  prep(diabetes_train) %>% 
  juice()                      # This step is optional.

diabetes_train_preprocessed %>% 
  dplyr::slice_sample(n = 10)

```

Some considerations. Extracting the pre-processed data with `juice()` is
not necessary since the prepped dataset will carry it during the machine
learning process. It is used under the hood.

With this done then, we can now specify which model to use for our
machine learning analysis.

### Specifying the model

After splitting the data into training and testing, identifying outcome
and predictors and knowing which steps to do pre-process our data, and
getting a pre-process dataset for that, now it is time to specify which
model will help us get the prediction of diabetes using the predictors
indicated.

For that the `parsnip` package aids in providing a unified interface for
the long list of models available in R.\
With one way to specify a model, we can then use this specification to
generate be it a linear model, a regression model, a random forest
model, or support vector machine (SVM), etc.

The full list of models can be searched in [the tidymodels
website.](https://www.tidymodels.org/find/parsnip/)

Some primary components will need to be taken into consideration when
specifying a model.

a.  The **model type**, which requires to specify the model we will use,
    for example, a random forest, or logistic regression, etc.
b.  The **arguments** or parameters to use within the model specified
    and that are consistent across different models. Set using
    `set_args()`
c.  The **engine** or underlying package where the model comes from
    (e.g. "ranger" for the range implemenation of Random Forest) using
    the `set_engine()` function.
d.  The **mode** refers to the type of prediction since many packages
    can do both classification (binary or categorical) or regression
    (continous prediction), set using `set_mode()`

In this notebook,the model type used can be the Random Forest for the
purpose of classification. The argument for this will be `mtry`
parameter to help us get the number of randomly selected variables to be
considered at each split in the trees. Then, we we would define the
following model specification:

```{r specifying a random forest model}
rf_model <- 
  parsnip::rand_forest() %>%                                   # specify that the model is random forest
  parsnip::set_args(mtry = tune()) %>%                         # specify that the `mtry` parameter needs to be tuned
  parsnip::set_engine("ranger", importance = "impurity") %>%   # select the engine/package that underlies the model
  parsnip::set_mode("classification")                          # choose either binary classification or continous regression
  
```

For examining the importance of the variables in the final model, we
need to add the `importance` argument. In the "ranger" package the
options for this argument are "impurity" an "permutation"

For instance, if instead of using a Random Forest model, we would like
to specify a Logistic regression model, we could do the following:

```{r specifying a linear regression model}
lr_model <- 
  parsnip::logistic_reg() %>%              # specify that the model is linear regression
  parsnip::set_engine("glm") %>%           # select the engine/package is "glm".  No set_args() needed
  parsnip::set_mode("classification")      # choose either binary classification or continuous regression


```

As can be seen in the last 2 specifications, no dataset has been passed
as argument. Which means that we are only building the framework to be
applied, not the actual analysis itself.\
In other words, we are not fitting any model yet since no data has been
passed on to it. Nothing in these models is specific to the diabetes
dataset.

These models could be defined way ahead of time to save time.

This just outlines a description of the model. Also, setting a parameter
to `tune()` means that it will be tuned later so that the parameter that
yield the best performance will be chosen.

## Putting it all together: The Workflow

After having gone through the previous steps, defining the recipes and
model, we are now ready to put this into a one workflow. Using the
`workflows` package.

First, the workflow needs to be initiated. After that we can then pass,
or add, the corresponding recipe and model to it.

```{r setting the workflow}
# setting the workflow for the random forest model
rf_workflow <- workflows::workflow() %>% 
  workflows::add_recipe(diabetes_recipe) %>%     # add the recipe
  workflows::add_model(rf_model)                 # add the model



```

Important to notice that we haven't implemented any pre-processing step
in the recipe nor have we fit the model to the data yet. Again we're
just writing the framework here.\
It is only when tuning parameters or actually passing the model to fit
that we will be indeed implementing the recipe and model framework
defined by the workflow.

If we take a look at the `rf_workflow` object we will see only the
structure of such framework.

```{r viewing the workflow framework}
rf_workflow 

```

### Tuning the parameters

In case we have parameters, these need to be tuned. Tuning means
choosing the value that leads to the best performance of our model. If
our model does not have parameters, this step can be skipped.

For our Random Forest model, the parameter we chose to tune was mtry.
Mtry includes a randomness in the choosing of the features to use to
create the decission trees of our model. A big number of features will
lead to having the decisions tress look the same, and a very small
number may ignore key features that impact our outcome variable. Mtry is
explained in very fine detail [in this blog
article.](https://crunchingthedata.com/mtry-in-random-forests/)

To obtain the value of mtry that we will use in our model/workflow, we
will use the cross-validation dataset `diabetes_cv` we created earlier.
The cross-validation data has lots of versions of our original dataset
that we can use to create our trees.

We first specify the values of mtry we want to try. We have in total 9
features (or variables or columns ) in our dataset. In this article, the
author used three possible numbers for mtry: 3, 4, 5.

Then we will try to get the metrics: [accuracy, and roc_auc, explained
via this
link](https://medium.com/nerd-for-tech/accuracy-vs-auc-roc-a8e7a384d153).

```{r tuning the mtry parameter, warning = FALSE}
rf_grid <- expand.grid(mtry = c(3, 4, 5))             # specify the values of mtry to try
rf_tune_results <- rf_workflow %>% 
  tune_grid(resamples = diabetes_cv,                  # The cross-validation object
            grid = rf_grid,                           # grid with values to try
            metrics = metric_set(accuracy, roc_auc)   # metrics we care about 
            )

```

It is a good idea to explore the results of the cross-validation.\
Using `collect_metrics()` from the `yardstick` package can be used to
extract the values of the metrics we input in the tuning phase (accuracy
and roc_auc)

```{r print results of the metrics values}
rf_tune_results %>% 
  collect_metrics()

```

According to the author, with `mtry = 4` we get the best performance
(*just*)

### Finalize the workflow

Now that the mtry parameter has been tuned. We need to add this into our
workflow so it chooses the best performing value. The function
`select_best()` will do the tuning for us and select the most performing
value.

```{r choosing the best performing value}
param_final <- rf_tune_results %>% 
  select_best(metric = "accuracy")

param_final

```

Then we add this final parameter to have our final workflow to use to
fit the dataset

```{r finalize the workflow}
rf_final_workflow <- rf_workflow %>% 
  finalize_workflow(param_final)

rf_final_workflow

```

## Evaluate the model on the test set

Once the recipe is defined, the model selected, and its parameters are
tuned, now we are ready to fit the final model.

Since all of the information is contained in the workflow object (the
final Random Forest workflow object `rf_final_workflow`) we will apply
the funcion `last_fit()` to that workflow and to our train/test split
object.

This will automatically train the model specified by the workflow using
the training data, and product the evaluations based on the dataset. The
function will know which dataset to use. Genius.

```{r fitting the training set and evaluate on the test set}
rf_fit <- rf_final_workflow %>% 
  last_fit(diabetes_split)       # fit on the training set and evaluate on the test set

rf_fit

```

The result above is a tibble with list columns. It can be worked using
`purrr`, since it is a data frame.

For example, let's extract the information in the metrics column. We
will use `dplyr::pull()`

```{r extract metrics column from fit object}
rf_fit %>% 
  dplyr::pull(.metrics)

```

```{r extract predictions column from fit object}
rf_fit %>% 
  dplyr::pull(.predictions)

```


### Extracting information from the fit model

However, there are functions within `tidymodels` to collect this. For
examples, we can callect the metrics using `collect_metrics()`. This
will extract the metrics calculated on the test set.

```{r using collect_metrics()}
test_performance <- rf_fit %>% 
  collect_metrics()

test_performance

```

The values for accuracy (0.78) and ROC_AUC (0.85) can be considered as
very good.

We can also get the predictions using `collect_predictions()`

```{r using collect_predictions}
test_predictions <- rf_fit %>% 
  collect_predictions()

test_predictions

```

The dimensions of the test_predictions table indicates 192 observations,
which is the number of observations of our test set.  
(`r nrow(diabetes_test)`)

Also, test_predictions is a data frame and we can do all sorts of calculations with it.

### Getting a confusion matrix

To create a confusion matrix we can use the function `conf_mat()`

```{r generate a confusion matrix}
test_predictions %>% 
  yardstick::conf_mat(truth = diabetes,
                      estimate = .pred_class)

```

### ROC_AUC Curve

To build a ROC_AUC, we can autoplot from yardstick.  We need to pick a truth column 
which is a factor, and one probability with the numbers that calculates our prediction. 

In our model we choose the .pred_neg column since it is the first one available in our 
set for left to right.  

```{r}
 autoplot(roc_curve(test_predictions,
                    diabetes, 
                    .pred_neg))
```

## Using the model

The purpose of creating a model that has good prediction performance is to use on 
new data.  

For that, we will need to create one final model that we will call `final_model` 
using our full diabetes data set (the clean version.)  The function to use is 
`fit()`

With that model, then we can use new data to get predictions.

```{r fitting the final model}
final_model <- fit(rf_final_workflow,
                   diabetes_clean)

final_model

```

### Predicting on new data

If we wanted to predict the diabetes status on a new woman, we can use the `predict()` 
function from the stats package.

```{r create the new woman dataset}
new_woman <- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,
                     2, 95, 70, 31, 102, 28.2, 0.67, 47)

new_woman

```

And now let's predict the result

```{r predict status on new woman}
final_model %>% 
  predict(new_data = new_woman)

```

The predicted diabetes status of this new woman is "negative".





















































